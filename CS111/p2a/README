NAME: Yuanping Song
EMAIL: yuanping.song@outlook.com
ID: 
SLIPDAYS: 1

The files that are included in my submission are as follows:

lab2-add.c: one of the source files of my program, implements the first part of 
the assignment. The program is capable of accepting command line options specifing the 
number of parallel threads used, yield or non-yield, as well as the synchronization 
mechanizism employed to facilitate performance testing. The threads increment a shared 
counter and race condition is demonstrated by a sometimes non-zero final count.  

lab2-list.c: One of the source files of my program, implements second part of the 
assignment. The program's command line options functions much like lab2-add's, with
the additional capability to specify more exactly where and how many yields take 
place. Parallel threads performs inserts, lookups, and deltes to a shared doubly 
linked list. Race condition is evidenced by a corrupted list.

*.png: graphs are included to illustrate the performance analysis described in this 
document
lab2_list-1.png: Cost per Operation vs Iterations
lab2_list-2.png: Unprotected Threads and Iterations that run without failure
lab2_list-3.png: Protected Iterations that run without failure
lab2_list-4.png: Scalability of synchronization mechanisms
lab2_add-1.png: Threads and Iterations that run without failure
lab2_add-2.png: The cost of yielding
lab2_add-3.png: Per operation cost vs number of iterations
lab2_add-4.png: Threads and Iterations that run without failure
lab2_add-5.png: per operation cost vs number of threads 

Makefile: script automating the building of the lab2 executables and the packaging 
of project files. Additionally runs the graph-making scripts and supports standard
make targets such as clean and dist. 

README: this file. Contains a description of the files included in 
the tarball as well as some general informaiton relating to my 
submission.

LIMITATIONS:
A race condition may occur as the program is exiting. The main thread
frees up all the memory acquired via malloc when the program exits using a
call to a cleanup function registered with atexit. However, other threads
might access the resources already freed by cleanup and cause a memory error.
I can fix this issue by not doing any memory freeing at all but I felt that
this issue occurs only rarely and does not impact the correctness of the
collected data so I felt it better to provide explicity memory management.
Question 2.1 - Causing Confilcts:

Why does it take many iterations before errors are seen?

	Errors will take place only when multiple threads are all attempting to increment or decrement the value of counter. Because creating a thread involves significant overhead as compared with the simple add routine, when the iteration count is low, it takes longer to instantiate a thread than for a thread to run. Therefore, the temporal overlap required for errors to been seen does not happen unless the threads each need to run many iterations.  

Why does a significantly smaller number of iterations so seldom fail?
	
	the scheduler typically chooses to run a thread immediately after it's created. Therefore, a child thread will be running the same time parent thread is instantiating other child threads. If the number of iterations is significantly smaller, it is very unlikely that the next child thread will start running before the previous child  finishes, or that the previous child would be preempted by the operating system and run at a later indeterminate time. So there is never any conflicting reads or writes to the counter and the program seldom fails. 


With only 2 threads, 750 iterations is the threshold where the addition starts to consistently fail. 

With 3 or more threads, the threshold reduces to approximately 500.

Question 2.1.2 - cost of yielding:
Why are the --yield runs so much slower?

The overhead of a thread yielding is very signficant as compared to just the arithmetic operations performed by the add function when run with out the --yield flag. This is because the operating system has to save a copy of the thread's environment, including its registers onto the kernel stack. Doing so every iteration is extremely expensive. Not to mention the thread is descheduled after the called to sched_yield(). Though it probably doesn't take very long for the thread to be reschduled, those two effects combined are responsible for the cost of yielding. 

Where is the additional time going? 
performance testing using the time command demstrates that while the yield flag causes user time to increase approximately ten fold, most of the additional time comes from system calls. It takes user program time to go through the extra conditional branch and set up the system call, but it is the OS that is doing the heavy lifting descheduling and rescheduling. 

Is it possible to get valid per-operation timing if we are using the --yield option?
Yes, it is still possible. Even the simple time command provides the user how much cpu time the lab2_add program uses. One can readily add user and system time to get this total. This excludes the time of threads spent waiting in the ready queue, and therefore is an accurate reflection of the total amount of work performed by the program. To get per operation time, simply divide the sum of user and sytem time by the number of operations. One can conceivably get a more accurate measurement by measuring the total time used by each thread within the lab2_add program by using he clock_gettime suite.

Question 2.1.3 - measurement errors:
Why does the average cost per operation drop with increasing iterations?
Because the effort by the parent thread to setup the child threads is average into the cost per operation. As the number of iterations increases, the total time is dominated by the work in the child threads and therefore is a closer and closer approximation to the actual amount of time needed per opteration.  

Because we know our approximation gets better and better with increasing numbers of iterations, we conceivably run as many iterations as we can afford to in each measrurement. Alternatively, we can keep increasing the number of iterations in our measurements, and we know that our average time per operation is sufficiently close to the true value when our computed time converges. 

Question 2.1.4 - costs of serialization

for low number of threads, the contention among threads for mutally exclusive access to the critical section is low. Therefore, there is not much difference in terms of performace between protected and unprotected operations. 

As the number of threads rises, each thread, on average, spends more time waiting due to contention and performance therefore decreases for protected operations.

Question 2.2.1 - scalability of Mutex

For the add program, cost per operation with mutex first increases linearly than evens off, even drops a bit. For the list program, the cost per operation increases exponentially without bound. (it looks linear but the graph's y axis is in log scale so the rate of growth is in fact exponential) The slope levels off in add because with enough threads, the processor is basically executing the critical section sequentially. So the cost per operation does not further increase with workload. The slope continues to increase in the list program because with more threads, the list grows bigger and there is more work to do per operation. Increasing wait time causes the cost to explode.  

Since the slope is positive in list and slightly negative in add for higher numbers of threads, the rate of increase of cost is clear greater for list. 

Question 2.2.2 - Scalability of spin locks

For both add and list, spin locks scales poorly with increasing number of threads. The shape of the curve is linear, slanting upward. However, we must note that the y-axis is in log scale where as the x-axis is linear. So the rate of growth in each graph is in fact exponential. This is the case because 1) for list, as before, more threads translate to bigger lists, more time per operations for the thread with the mutex as well as much longer waittime for threads without. These two factors causes the cost to explode. 2) in add, because spin lock causes a thread waiting for the lock to consume its entire time slice, wait time per critical section is proportional to the number of threads, that combined with the fact that there are now more threads to wait on also causes the cost to explode. 

The rate of growth in list is slightly higher because in that program, waittime per critical section is not only proportional to the number of threads but also to the length of the list, so effectively the number of threads squared.   

