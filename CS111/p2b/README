NAME: Yuanping Song
EMAIL: yuanping.song@outlook.com
ID: 
SLIPDAYS: 0

The files that are included in my submission are as follows:

lab2_list.c: The source of the program on which the performance analysis is carried out.
The program accepts command line options to facilitate testing and outputs data in CSV.
Internally, the program benchmarks standard list operations. Key features are profiler
support and list partitioning.

Sortedlist.c/Sortedlist.h: interface and implementation for a circular doublely linked list with dummy node.

*.gp: scripts for generating the lists described below.

*.png: graphs are included to illustrate the performance analysis described in this 
document
lab2b_1.png: Throughput vs Threads
lab2b_2.png: Avg time vs Threads
lab2b_3.png: Multiple List with and w/o Synchronization
lab2b_4.png: Mutex Protected Partitioned List Performance
lab2b_5.png: Spin Lock Protected Partitioned List Performance

profile.out: data file generated by google profiler.

lab2b_list.csv: test data file.

Makefile: script automating the building of the lab2 executables and the packaging 
of project files. Additionally runs the graph-making scripts and supports standard
make targets such as clean and dist. 

README: this file. Contains a description of the files included in 
the tarball as well as some general informaiton relating to my 
submission.



Works Consulted:
I consulted the GNU website for how to compute the difference between to timespec structures and used one of their published solutions timeval_subtract: https://www.gnu.org/software/libc/manual/html_node/Elapsed-Time.html




QUESTION 2.3.1 - CPU time in the basic list implementation:
a) Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests ?
Since the a small number of threads would not have a lot of contension, I expect most of the CPU time to be spent doing actual list operations.

b) Why do you believe these to be the most expensive parts of the code?
I believe that my list operation code is most expensive because the list, which has either one or two thousand elements in it, is quite large. In comparison, the critical sections are short so the extent of contention is low.

c)Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
I believe most of the time is spent spinning because spin-locks are not preemptive. So if a thread does not get a lot, it usually wastes an entire time slice. However, since acquiring and releasing lock is cheap, this does not necessarily translate to worse performance as compared to mutexes because the departmental server has a large number of cores.

d) Where do you believe most of the CPU time is being spent in the high-thread mutex tests?
I believe most of the time is spent in context switches because all but one thread are blocked and those blocked threads immediately gives up their time slices. so the threads may in the worst case, run in a round robin fashion, wasting most time in expensive context switches. 

QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads?
The vast majority of the time is spent acquiring the lock in loops such as:
while(__sync_lock_test_and_set(&spin_lock, 1) == 1){
             // spin
}

Why does this operation become so expensive with large numbers of threads?
this operation becomes very expensive because with a large number of threads,
only a single one of them can be in the critical section at the same time. The rest just waits and spins. The performance degradation is especially serious with spin locks because it is not premptive. And if the machine does not have enough cores, convoy can cause .
wate time to explode. 

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Beause spin lock is not premptive, the lock wait time is proportional to the product of the number of contending threads as well as the amount of time needed to perform the list operations. The later depends on the length of the list which in turn depends on the number of threads. Therefore, the wait time increases superlinearly with increasing number of threads.
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
Because one of the threads is almost always doing useful work when it acquires the lock and also because the amount of work per operation is proportional to the length of list which only grows linear with increasing number of threads. So the completion time grows at a slower rate
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
This is possible since the completion time is measured per process, and the wait time is measured per thread and then sumed over all threads. When the number of threads increases, it is entirely permissible for the waittime to increase at a higher rate than the completion time.

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.

Performance initially drops as the thread count increases from 2 to 4 due to increased contention. the drop is precipitous because there is a big penalty to performance if a thread needs to wait frequently. Next, the curve slopes slightly upward and reaches a local maximum as the thread count approaches the number of lists. Thereafter, throughput again decreases, this time much more slowly. This is because multiple lists greatly reduces convoy effect as well as the time spent waiting, keeping the system performant under load.

Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
The performance will increase for a time. But ultimately, the performance gains of using multiple lists is limited by the overhead involved, as well as the cost of operations that must lock the entire list. Namely, the step where the length of the entire list has to be computed. This operation is very expensive because all of the locks has to be acquired by a single thread, which kills parallelism. 
 
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.
This is an imperfect approximation. Actual partitioned lists perform worse because despite the partitioning, the operations are not guaranteeded to be distributed evenly among the partitions so threads spend more time waiting than in this ideal case.
